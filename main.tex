\documentclass[letterpaper]{article}

\usepackage{natbib,alifeconf}  %% The order is important
\usepackage{url,hyperref,amsmath,cleveref}
\usepackage{booktabs}

% *****************
%  Requirements:
% *****************
%
% - All pages sized consistently at 8.5 x 11 inches (US letter size).
% - PDF length <= 8 pages for full papers, <=2 pages for extended
%    abstracts (not including citations).
% - Abstract length <= 250 words.
% - No visible crop marks.
% - Images at no greater than 300 dpi, scaled at 100%.
% - Embedded open type fonts only.
% - All layers flattened.
% - No attachments.
% - All desired links active in the files.

% Note that the PDF file must not exceed 5 MB if it is to be indexed
% by Google Scholar. Additional information about Google Scholar
% can be found here:
% http://www.google.com/intl/en/scholar/inclusion.html.


% If your system does not generate letter format documents by default,
% you can use the following workflow:
% latex example
% bibtex example
% latex example ; latex example
% dvips -o example.ps -t letterSize example.dvi
% ps2pdf example.ps example.pdf


% For pdflatex users:
% The alifeconf style file loads the "graphicx" package, and
% this may lead some users of pdflatex to experience problems.
% These can be fixed by editing the alifeconf.sty file to specify:
% \usepackage[pdftex]{graphicx}
%   instead of
% \usepackage{graphicx}.
% The PDF output generated by pdflatex should match the required
% specifications and obviously the dvips and ps2pdf steps become
% unnecessary.


% Note:  Some laser printers have a serious problem printing TeX
% output. The use of ps type I fonts should avoid this problem.


\title{Conchordal: Emergent Harmony via Direct Cognitive Coupling in a Psychoacoustic Landscape}


% Each submission will undergo a double-blind review process. To this end, submissions should NOT contain any element that could reveal the identity of the authors (author names, affiliations, funding details and acknowledgments), and should use the third person to refer to previous work by the authors.
\author{
    Koichi Takahashi$^{1}$$^{2}$
    \mbox{}\\
    $^1$Graduate School of Media and Governance, Keio University \\
    $^2$Conchordal.org \\
    info@conchordal.org
} % email of corresponding author

% For several authors from the same institution use the same number to
% refer to one address.
%
% If the names do not fit well on one line use
%         Author 1, Author 2 ... \\ {\Large\bf Author n} ...\\ ...
%
% If the title and author information do not fit in the area
% allocated, place \setlength\titlebox{<new height>} after the
% \documentclass line where <new height> is 2.25in

\newcommand{\optionalpaperfigure}[2]{%
\IfFileExists{#1}{%
\includegraphics[width=#2]{#1}%
}{%
\fbox{\parbox[c][0.22\textheight][c]{0.9\columnwidth}{\centering
Missing figure:\\\texttt{\detokenize{#1}}}}%
}%
}




% Choose one of: Full Paper, Summaries, or Late Breaking Abstracts 
% Submission type: \textbf{Full Paper}\\

% If sharing code / data, anonymize your repository and paste the link here.
% Example of anonymizing sevice for github: https://anonymous.4open.science/
% delete this line if not needed
% Data/Code available at: \url{http://your.repo.here.com}



\begin{document}
\maketitle

\begin{abstract}
\noindent Conchordal is a novel system that treats sound as an adaptive lifeform, merging psychoacoustic principles with artificial life (ALife) techniques to produce emergent music. Unlike traditional generative music approaches that manipulate symbolic notes on fixed-tempered grids, Conchordal operates directly on a continuous acoustic spectrum aligned with human hearing. The system introduces a perceptual coordinate space, real-time auditory feedback through roughness and harmonicity analysis, and autonomous virtual ``Individual'' agents that seek spectral \textit{consonance} as a survival fitness. The result is an ecosystem of sound where harmony, rhythm, and timbre self-organize in real time, without any pre-composed score. We detail the core components of the Conchordal architecture and demonstrate how its bio-acoustic paradigm leads to novel emergent musical structures. 
\end{abstract}

\section{Introduction}
Generative music systems have traditionally relied on manipulating symbolic representations of notes and beats (e.g., MIDI pitches and fixed meter) within a predefined framework. In contrast, \textit{Conchordal} diverges from these norms by functioning as a continuous, biologically-inspired simulation of auditory perception. Rather than compose with discrete pitches and rhythms, Conchordal treats the audio spectrum itself as a physical environment in which virtual agents live. Musical structure is not explicitly composed but emerges as an \emph{adaptive property} of agents striving to ``survive'' in a hostile sonic world.

Each agent in the system perceives and acts upon a shared auditory \textit{landscape}, analogous to an ecosystem. Sound is modeled as a living entity—an \emph{Individual}—with metabolism, sensory processing, and autonomous behavior. The driving goal for all agents is to maximize their \emph{spectral consonance}, a measure of auditory comfort. Consonance is defined by combining two psychoacoustic factors: low sensory \emph{roughness} (minimal dissonance from beating tones) and high \emph{harmonicity} (strong perception of a stable virtual pitch). Rather than letting these factors cancel linearly, Conchordal first defines an additive perceptual \emph{dissonance} cost and then maps it to a bounded consonance score:
\begin{equation*}
\begin{aligned}
D(f) &= \alpha\,\bigl(1 - H(f)\bigr) + \bigl(w_0 + w_1\,(1 - H(f))\bigr)\,R(f)\,,\\
C(f) &= \frac{1}{1 + D(f)}\,.
\end{aligned}
\end{equation*}
Here $R(f)$ and $H(f)$ are normalized to the range $[0,1]$, and $\alpha$, $w_0$, and $w_1$ control the relative contribution of harmonicity deficit and roughness. This definition captures the classic notion that pleasant sonorities minimize rapid amplitude beating while reinforcing harmonic series relationships \citep{Helmholtz1885, Plomp1965}. Agents continuously analyze their acoustic environment to maximize this consonance score by adjusting their emitted frequencies and rhythms.

The result is a self-organizing soundscape where harmony, rhythm, and timbre evolve organically through local interactions, rather than following a predetermined score. Similar artificial-life approaches to music have hinted at such emergent sonic ecosystems \citep{Bilotta2001, Dahlstedt2001, Miranda2002a}, but Conchordal is unique in grounding the entire system in human \emph{psychoacoustics}. In this paper, we present the design and implementation of Conchordal, highlighting three foundational pillars of the architecture: (1) a psychoacoustic coordinate system that aligns the simulation with human hearing, (2) a real-time auditory analysis pipeline that computes roughness and harmonicity fields from sound, and (3) an agent-based ``life engine'' that drives musical evolution in the spectral environment.

\section{Psychoacoustic Coordinate System}
A critical innovation in Conchordal is the rejection of the linear Hertz scale for internal frequency representation. Human pitch perception is inherently \emph{logarithmic}: the ear perceives equal-ratio frequency steps (octaves or intervals) as musically equidistant, rather than equal absolute differences in Hz \citep{Terhardt1974, Moore1990}. To reflect this, Conchordal establishes a custom logarithmic frequency space, called \texttt{Log2Space}, which directly maps physical frequency $f$ (Hz) to a perceptual pitch axis $l$:
\[ l = \log_{2}(f)~, \qquad f = 2^{\,l}~. \] 
In this base-2 logarithmic domain, an increment of 1.0 corresponds exactly to one octave—arguably the most fundamental interval in music perception. The internal simulation grid is quantized into a constant step $\Delta l$, typically $1/48$ or $1/96$ of an octave, providing sub-semitone resolution for smooth microtonal pitch changes. This framework aligns the system’s notion of “distance” between frequencies with the tonotopic organization of the human cochlea.

By design, the \texttt{Log2Space} grid yields a \textit{constant-$Q$} frequency resolution. The term $Q = f/\Delta f$ denotes the quality factor of a frequency band. In standard Fourier analysis with fixed bandwidth, $Q$ increases at higher frequencies (bands get relatively narrower), whereas in our logarithmic spacing $\Delta f$ grows proportionally with $f$, keeping $Q$ approximately constant across the spectrum. This mimics the frequency selectivity of human hearing: our ability to distinguish tones is roughly proportional to frequency (e.g., a 20~Hz difference is significant at 200~Hz but negligible at 2000~Hz) \citep{Moore1990}. An immediate benefit is computational efficiency—Conchordal can maintain uniform perceptual resolution without resorting to separate low- and high-frequency processing stages, since the analysis inherently uses shorter time windows at high frequencies and longer windows at low frequencies.

However, a purely logarithmic scale does not fully capture the ear’s actual critical bands, which are somewhat broader at very low frequencies than a strict octave-based spacing would predict. To address this, Conchordal incorporates the \emph{Equivalent Rectangular Bandwidth} (ERB) scale as a secondary mapping for certain calculations. The ERB scale, derived from psychoacoustic measurements \citep{Glasberg1990}, defines the effective bandwidth of human auditory filters at different center frequencies. For example, the ERB-rate transformation for frequency $f$ (in Hz) can be given by:
\[ E(f) = 21.4 \log_{10}(0.00437\,f + 1)\,, \] 
and the corresponding ERB bandwidth is approximately 
\[ \text{BW}_{ERB}(f) \approx 24.7\, (0.00437\,f + 1)~\text{Hz}. \] 
Conchordal leverages this psychoacoustic scaling when evaluating dissonance: it maintains a dual view of the spectrum where logarithmic \texttt{Log2Space} is used for pitch/harmony computations, while the ERB scale is used to measure interference and roughness. By aligning its internal frequency representation with known properties of human hearing, the system grounds its musical decisions in perceptual reality.

\begin{figure}[t]
\centering
%\includegraphics[width=0.85\columnwidth]{placeholder1.png}
\caption{Logarithmic frequency mapping in Conchordal. The \texttt{Log2Space} coordinate system ensures that equal distances correspond to equal musical intervals (octaves), providing a perceptual foundation for the simulation. A linear Hertz scale (top) vs. the Log2 scale (bottom) illustrates how an octave span (e.g., 250 Hz to 500 Hz vs. 1000 Hz to 2000 Hz) is represented by equal steps in the internal model.}
\label{fig:logspace}
\end{figure}

\section{Auditory Analysis: The Spectral Landscape}
In Conchordal, all agent interactions occur through a shared computational structure called the \textit{Landscape}. This landscape is a dynamic representation of the current sound environment, continuously updated by a real-time digital signal processing (DSP) pipeline. It can be envisioned as a set of scalar fields defined across frequency (and over time) that quantify the ``fitness'' of each region of the spectrum. Specifically, at each analysis time step the system computes two primary fields on the Log2Space grid:
\begin{itemize}\itemsep0pt
    \item \textbf{Roughness ($R$)}: a measure of sensory dissonance or ``harshness'' caused by closely spaced spectral components beating against each other.
    \item \textbf{Harmonicity ($H$)}: a measure of tonal stability, reflecting the strength of a clear fundamental (virtual pitch) in the local spectrum.
\end{itemize}
These measures draw on established psychoacoustic research: roughness relates to the phenomenon of critical band interference \citep{Plomp1965}, whereas harmonicity corresponds to the perceptual detection of harmonic series patterns in sound \citep{Terhardt1974}. Both $R$ and $H$ are normalized to a 0--1 range. The consonance metric $C$ introduced earlier is computed by first accumulating an additive dissonance cost $D$ from $(1-H)$ and $R$, and then applying a bounded mapping $C = 1/(1 + D)$ at each frequency bin (or for an aggregate sound), indicating how congenial that spectral region is for the agents.

Concretely, letting $A$ denote the analyzed magnitude spectrum, we define roughness via an ERB-domain convolution and a saturating normalization,
\begin{equation*}
\begin{aligned}
R(f) &= \mathcal{N}_R\!\left( \int A_{ERB}(\tau)\,K_{rough}\!\bigl(|E(f)-\tau|\bigr)\,d\tau \right)\,,
\end{aligned}
\end{equation*}
where $E(\cdot)$ maps Hz to ERB-rate and $K_{rough}$ is the Plomp--Levelt interference kernel.

We define harmonicity through a two-pass ``sibling projection'' (virtual-root detection),
\begin{equation*}
\begin{aligned}
\mathrm{Roots}(f) &= \sum_{k=1}^{K} w_k\,A(k f)\,,\\
H(f) &= \mathcal{N}_H\!\left( \sum_{m=1}^{M} w_m\,\mathrm{Roots}(f/m) \right)\,,
\end{aligned}
\end{equation*}
where $w_k$ and $w_m$ are decaying weights over harmonic indices, and $\mathcal{N}_R$ and $\mathcal{N}_H$ map to $[0,1]$.

To derive these fields from the raw audio stream, Conchordal employs a multi-stage analysis. First, the input sound (the mix of all agents' output) is transformed into the frequency domain using a \emph{Non-Stationary Gabor Transform} (NSGT). The NSGT is a variant of the short-time Fourier transform that allows variable resolution: longer analysis windows at low frequencies and shorter windows at high frequencies, perfectly complementing the constant-$Q$ Log2Space grid. In practice, a single fast Fourier transform (FFT) on the audio buffer is combined with a bank of precomputed spectral kernels to produce a logarithmically spaced spectrum covering the full 20~Hz–20~kHz range. This design yields an efficient analysis-by-synthesis scheme: rather than perform hundreds of small FFTs or a complex wavelet transform, Conchordal does one FFT and projects it onto the Log2Space basis \citep[cf.][]{Brown1992constantQ}. A subsequent smoothing stage applies an exponential moving average to each frequency band (with frequency-dependent time constants) to simulate the finite integration time of human hearing and to stabilize the landscape for the agents.

Next, the system calculates roughness $R$ by evaluating how much spectral energy lies within each critical band and how these components interact. Conchordal implements a variant of the Plomp and Levelt roughness model \citep{Plomp1965} in the ERB domain. In essence, the algorithm convolves the spectrum (expressed on an ERB-frequency axis) with a specific \textit{roughness kernel} that quantifies dissonance as a function of frequency separation. Pairs of partials that are very close in frequency produce a lot of roughness (rapid beating), which peaks at a certain separation (around a quarter of an ERB as per psychoacoustic data) and then diminishes for wider separations (which the ear resolves as distinct tones rather than rough beats). By performing this convolution efficiently, the system obtains a roughness ``landscape'' across frequencies without explicitly comparing every spectral component pair, avoiding $O(N^2)$ complexity.

Harmonicity $H$ is computed via a complementary analysis that looks for harmonic series patterns in the spectrum. Each agent can produce complex tones with multiple partials, and the system attempts to infer the presence of an underlying fundamental frequency. Conchordal’s \textit{sibling projection} algorithm projects observed spectral peaks onto hypothetical harmonic templates (series of frequencies at integer ratios). If a set of frequencies fits well onto such a template (for example, frequencies that align with multiples of some base frequency $f_0$), it indicates a strong virtual pitch at $f_0$ and thus high $H$ in that region. This approach is related to classic virtual pitch and pitch detection models in psychoacoustics \citep{Terhardt1974}, but here it is applied continuously over the spectrum to yield a ``harmonicity field.'' Regions of the landscape with high $H$ correspond to tonal centers or stable chords, whereas regions with low $H$ (inharmonic collections of frequencies) appear perceptually ambiguous or noise-like.

At this point we define \textbf{Mirror Dualism} as a binding-rule continuum in sibling projection, not as a major/minor label by itself. The down$\rightarrow$up projection corresponds to \textit{common-root binding} (overtone direction), while the up$\rightarrow$down projection corresponds to \textit{common-overtone binding} (undertone direction). A single control parameter, \texttt{mirror\_weight} $\in [0,1]$, continuously blends these two hypotheses. Under this interpretation, a major-like stack such as $4\!:\!5\!:\!6$ is read as a strong root-binding instance, whereas a minor-like companion such as $10\!:\!12\!:\!15$ is read as a strong ceiling-binding instance. Major/minor naming is therefore an interpretation aid; the primary mechanism is the direction of spectral binding.

Finally, the roughness and harmonicity information are combined. For each frequency bin, a consonance score $C$ is derived from $H$ and $R$ as described. This consonance field effectively tells the agents which areas of the spectrum are currently hospitable (high $C$ indicates tonal, non-dissonant niches) and which are hostile (low $C$ indicates clashing, rough areas). Importantly, agents do not interact with each other directly; instead, they all read from and write to this common landscape. An agent “senses” the local $R$, $H$, and $C$ values in its vicinity and makes decisions, and when it produces a sound, it writes energy into the landscape (affecting those fields for others). This indirect interaction via the environment ensures scalability (interaction complexity is linear in the number of agents) and creates a feedback loop where the agents collectively shape the spectral terrain that in turn drives their adaptive behavior.

\begin{figure}[t]
\centering
%\includegraphics[width=0.9\columnwidth]{placeholder2.png}
\caption{Auditory ``landscape'' representation. This schematic spectrogram illustrates how the system evaluates Roughness $R$ and Harmonicity $H$ across the frequency spectrum. Peaks that are closely spaced in frequency (within the same critical band) contribute to high roughness (red regions in the $R$ field), while sets of peaks forming harmonic series yield high harmonicity (blue regions in the $H$ field). Agents sense these fields and gravitate toward frequency regions of high consonance $C$ (low $R$, high $H$).}
\label{fig:landscape}
\end{figure}

\section{The Life Engine: Autonomous Musical Agents}
Layered on top of the spectral landscape is Conchordal’s \textit{Life Engine}, an agent-based simulation that brings the ecosystem to life. Each agent (or \textit{Individual}) in Conchordal is an autonomous synthesizer with its own internal state and behavior routines. These agents are the source of all sounds in the system: they ``live'' by continuously emitting audio (tones or complex sounds) and ``listening'' to the resulting mixture via the landscape feedback. Their goal, simply put, is to inhabit the spectrum in a comfortable way—finding frequency regions and patterns that maximize their consonance fitness.

Every Individual has a modular architecture inspired by principles of ALife and behavior-based robotics. Key components include:
\begin{itemize}\itemsep0pt
    \item \textbf{SoundBody}: the sound production mechanism (voice) of the agent. This can be a simple sinusoidal oscillator or a more complex harmonic oscillator with a configurable timbre. The SoundBody defines the agent’s basic sonic characteristics (pitch range, tone color) and projects the agent’s output into the spectral grid \citep[cf.][]{Moroni1990}.
    \item \textbf{Perception \& Pitch Controller}: the agent’s sensing unit that monitors the local roughness and harmonicity in the landscape and decides what frequency to produce next. It includes a \emph{perceptual context} module that tracks recently played pitches and their outcomes (e.g., whether they led to good consonance or boredom from repetition), somewhat akin to a short-term memory or habituation mechanism.
    \item \textbf{Phonation Engine}: a timing control unit that governs when the agent produces sound (note onsets, durations, and pauses). Each agent has an internal rhythmic clock; interestingly, these clocks can influence each other via the shared sound environment, leading to spontaneous synchronization of tempos.
    \item \textbf{Lifecycle \& Metabolism}: parameters that handle the agent’s lifespan and energy. Agents might slowly lose ``energy'' when sounding in highly dissonant conditions and potentially ``die'' if they consistently fail to find consonant niches, making room for new agents or mutations (depending on the experimental setup). This adds an evolutionary pressure to the system.
\end{itemize}

At every simulation tick, an agent evaluates its current state: it reads the landscape at its current frequency (and neighboring frequencies) to gauge $R$, $H$, and $C$. Based on this and its internal rules, it may adjust its pitch (to climb towards a more consonant region or to explore), change its loudness or timbre, or enter a silent state if the environment is too hostile. The pitch control often follows a heuristic akin to hill-climbing on the $C(f)$ field: an agent will tend to drift toward frequencies where $C$ is higher (less rough, more harmonic) unless constrained by its pitch range or a novelty-seeking bias. In our implementation, this drift includes an explicit movement cost term with configurable coefficient (\texttt{move\_cost\_coeff}) and distance exponent (\texttt{move\_cost\_exp} $\in \{1,2\}$), allowing linear or quadratic penalties for larger pitch jumps. However, to prevent all agents from converging to the same tone, the perceptual context module induces a form of exploratory behavior—agents can become ``bored'' if they stay too long in one comfortable spot, prompting them to seek new sonic territory.

The timing of each agent’s outputs is managed by its PhonationEngine. Conchordal uses an elegant mechanism to model rhythmic entrainment: agents’ internal oscillators for timing can synchronize through sound. For example, one variant of the articulation core uses a Kuramoto-style coupled oscillator model \citep{Kuramoto1984} so that when multiple agents produce periodic pulses, their clocks tend to lock phase if their frequencies are similar. In practice, if one agent is pulsing at, say, 120~beats per minute and another at 125~bpm, over time they may synchronize at a mutually agreeable rate or phase due to the coupling induced by hearing each other’s beats. This phenomenon has been observed in human rhythmic coordination and is effectively simulated here in a simple form. The outcome is emergent \emph{polyrhythms} and synchronized grooves without any central clock, purely as a result of interaction \citep[cf.][]{Dahlstedt2001}.

\begin{figure}[t]
\centering
%\includegraphics[width=0.85\columnwidth]{placeholder3.png}
\caption{Agent architecture and interaction. Each Individual agent contains (A) a SoundBody that generates audio, (B) a Perception/Pitch Control module that senses the landscape’s roughness/harmonicity and chooses frequency actions, and (C) a Phonation engine that sets rhythmic patterns. Agents influence each other only by altering the shared spectral landscape; for instance, one agent’s output can raise the roughness in a region, deterring others from that band. Through this indirect coupling, coherent musical structures (consonant chords, synchronized rhythms) emerge from purely local decisions.}
\label{fig:agent}
\end{figure}

The agents in Conchordal collectively exhibit a form of open-ended musical evolution. There is no preset score or target melody; instead, patterns form and dissolve based on the agents’ ongoing adaptive behaviors. For example, a group of agents might spontaneously assume stable roles that resemble a musical chord: one settles into a low-frequency drone providing a tonal center (high $H$, low $R$ there), while others occupy higher-frequency tones at intervals that minimize roughness with the drone (e.g., a perfect fifth, which is known to be low in beating dissonance \citep{Plomp1965}). If one agent shifts or a new agent enters, the whole configuration readjusts. The system has no explicit rules for musical harmony, yet it recreates aspects of tonal music theory (consonant intervals, chord structures) through the lens of perceptual physics and survival dynamics.

\section{Experiments (E1--E5)}
We implemented Conchordal as a real-time Rust simulation with multi-threaded analysis, control, and synthesis, and evaluated it through five reproducible experiments (E1--E5). All experiment plots are emitted to \texttt{examples/paper/plots/e*}. For manuscript builds, each panel below references that output path directly (with a fallback box if the file is not present yet).

\subsection*{E1: Landscape Scan}
E1 validates the static auditory landscape itself: roughness/harmonicity-derived consonance structure around a fixed anchor.
\begin{figure}[t]
\centering
\optionalpaperfigure{examples/paper/plots/e1/paper_e1_landscape_scan_anchor220.pdf}{0.95\columnwidth}
\caption{E1 landscape scan around the anchor.}
\label{fig:e1_landscape_scan}
\end{figure}

\subsection*{E2: Emergent Harmony}
E2 tests whether independent agents improve consonance over time and form interval fingerprints under shared landscape coupling.
\begin{figure}[t]
\centering
\optionalpaperfigure{examples/paper/plots/e2/paper_e2_figure_e2_1.pdf}{0.95\columnwidth}
\caption{E2-1: temporal improvement in consonance-related observables.}
\label{fig:e2_main}
\end{figure}

\begin{figure}[t]
\centering
\optionalpaperfigure{examples/paper/plots/e2/paper_e2_figure_e2_2.pdf}{0.95\columnwidth}
\caption{E2-2: interval histogram and consonant-mass summaries.}
\label{fig:e2_hist}
\end{figure}

\subsection*{E3: Metabolic Selection}
E3 evaluates survival dynamics under consonance-dependent metabolic pressure.
\begin{figure}[t]
\centering
\optionalpaperfigure{examples/paper/plots/e3/paper_e3_firstk_survival_compare_pooled.pdf}{0.95\columnwidth}
\caption{E3 survival curve under metabolic selection.}
\label{fig:e3_survival}
\end{figure}

\subsection*{E4: Mirror Dualism Sweep}
E4 is the central mirror experiment and is interpreted as a \textbf{Root-binding $\leftrightarrow$ Ceiling-binding regime shift}. The primary order parameter is
\[
\Delta_{\mathrm{bind}} = \frac{\mathrm{RootFit} - \mathrm{CeilingFit}}{\mathrm{RootFit} + \mathrm{CeilingFit} + 10^{-6}}.
\]
Protocol-wise, E4 fixes anchor frequency, exploration range, minimum distance, timbre, and population size across conditions; only \texttt{mirror\_weight} is swept, with multi-seed repetition for confidence intervals.
Here the claim is not ``major to minor'' per se. Instead, \texttt{mirror\_weight} controls which binding rule dominates sibling projection. Major/minor statistics are retained only as interpretation aids in supplementary analysis.

\begin{figure}[t]
\centering
\optionalpaperfigure{examples/paper/plots/e4/paper_e4_bind_vs_weight.pdf}{0.95\columnwidth}
\caption{E4 main panel: $\Delta_{\mathrm{bind}}$ vs \texttt{mirror\_weight} (mean and 95\% CI across seeds).}
\label{fig:e4_bind_main}
\end{figure}

\begin{figure}[t]
\centering
\optionalpaperfigure{examples/paper/plots/e4/paper_e4_interval_fingerprint_heatmap.pdf}{0.95\columnwidth}
\caption{E4 interval fingerprint heatmap over mirror-weight sweep.}
\label{fig:e4_heatmap}
\end{figure}

\begin{figure}[t]
\centering
\optionalpaperfigure{examples/paper/plots/e4/paper_e4_step_response_delta_bind.pdf}{0.95\columnwidth}
\caption{E4 step response (optional supporting panel): $\Delta_{\mathrm{bind}}(t)$ after mirror-weight switch.}
\label{fig:e4_step}
\end{figure}

To avoid terminology collision, the prior timbre-mirroring scene is renamed to \textbf{Spectral Mirror Dialogue} (supplementary qualitative demo), while ``Mirror Dualism'' is reserved for the binding-rule meaning above.

\subsection*{E5: Rhythmic Entrainment}
E5 quantifies rhythmic coupling through phase-locking and phase-difference observables.
\begin{figure}[t]
\centering
\optionalpaperfigure{examples/paper/plots/e5/paper_e5_plv_over_time.pdf}{0.95\columnwidth}
\caption{E5 PLV dynamics under entrainment coupling.}
\label{fig:e5_plv}
\end{figure}

\begin{figure}[t]
\centering
\optionalpaperfigure{examples/paper/plots/e5/paper_e5_delta_phi_over_time.pdf}{0.95\columnwidth}
\caption{E5 phase-difference trajectory under kick/control conditions.}
\label{fig:e5_phase}
\end{figure}

\section{Conclusion}
We presented Conchordal, a novel artificial life framework for generative music that operates directly on the principles of auditory perception. By unifying psychoacoustic modeling (critical bands, virtual pitch, neural oscillation) with an ecosystem of adaptive sound-producing agents, Conchordal generates music as a byproduct of a simulated struggle for consonant sound. This approach contrasts with conventional music generation techniques by eschewing explicit music-theoretic rules and instead allowing musical structures to emerge from perceptual and survival constraints. 

The implications of this work are twofold. First, it offers a new paradigm for algorithmic composition and sound design: composers can steer or initialize the system, but the fine details of the music are evolved in real-time by the system itself responding to perceptual criteria. Second, from a scientific perspective, Conchordal provides a sandbox to explore how complex, life-like dynamics can manifest in the spectral domain, potentially offering insights into questions about the origins of musical consonance and the self-organization of rhythm in nature. Future work will deepen the adaptive capabilities of the agents (e.g., incorporating learning or communication between individuals) and explore integrating human performers with the ecosystem. We hope this bio-acoustic ALife approach will inspire further interdisciplinary research linking music, psychology, and artificial life.

\footnotesize
\bibliographystyle{apalike}
\bibliography{conc}

\section{Acknowledgements}
This work was supported by NSF grant No.\ 123456.

\end{document}
